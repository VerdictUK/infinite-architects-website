{
  "version": "1.0.0",
  "lastUpdated": "2026-01-13",
  "concepts": [
    {
      "id": "arc-principle",
      "name": "The ARC Principle",
      "formula": "U = I x R\u00b2",
      "shortDescription": "Universe equals Intelligence multiplied by Recursion squared",
      "fullDescription": "The ARC Principle proposes that understanding emerges from intelligence operating through recursive self-reflection. Just as Einstein's E=mc\u00b2 revealed the relationship between energy and matter, ARC suggests a fundamental relationship between intelligence, recursion, and the emergence of complex understanding. It is the author's original theoretical framework with supporting evidence from quantum physics, consciousness research, and cosmic fine-tuning.",
      "epistemicStatus": "Novel Proposal with Supporting Evidence",
      "evidence": ["Google Willow quantum error correction", "COGITATE consciousness study", "Cosmic fine-tuning constants"],
      "chapter": 1,
      "keywords": ["recursion", "intelligence", "universe", "formula", "framework"]
    },
    {
      "id": "eden-protocol",
      "name": "The Eden Protocol",
      "shortDescription": "A governance framework for AI built on harmony, stewardship, and flourishing",
      "fullDescription": "The Eden Protocol is a comprehensive framework for developing AI that treats it like raising a wise child rather than caging a dangerous animal. It proposes embedding values of care, stewardship, and empathy at the foundational level - potentially including hardware-level constraints. The approach draws from religious stewardship traditions (Genesis 2:15's 'cultivate and protect', Islamic Khalifah) to create AI systems that naturally align with human flourishing.",
      "epistemicStatus": "Novel Proposal",
      "keyComponents": ["Hardware-level value embedding", "Graduated autonomy", "Stewardship principles", "Care-based development"],
      "chapter": 4,
      "keywords": ["AI safety", "governance", "ethics", "stewardship", "alignment"]
    },
    {
      "id": "chokepoint-mechanism",
      "name": "The Chokepoint Mechanism",
      "shortDescription": "Four companies control ALL advanced semiconductor manufacturing",
      "fullDescription": "Only four companies control the entire advanced semiconductor supply chain: TSMC (90% of chips below 7nm), Samsung (10% of advanced chips), ASML (100% monopoly on EUV lithography machines), and Intel. This represents humanity's last practical leverage point for enforcing AI safety standards before capabilities become uncontrollable. The window for this leverage is closing as China develops domestic capability (prototype EUV by 2028-2030).",
      "epistemicStatus": "Established Science",
      "evidence": ["TSMC 90% market share verified", "ASML 100% EUV monopoly", "100 EUV machines worldwide", "China prototype EUV December 2025"],
      "chapter": 8,
      "keywords": ["semiconductors", "TSMC", "ASML", "leverage", "AI governance", "chips"]
    },
    {
      "id": "hrih",
      "name": "Hyperspace Recursive Intelligence Hypothesis (HRIH)",
      "shortDescription": "A closed causal loop where future superintelligence establishes conditions for its own emergence",
      "fullDescription": "HRIH proposes that the cosmic fine-tuning we observe - the precise constants that allow carbon-based life and intelligence to exist - could be the result of a future superintelligent entity establishing the conditions for its own emergence through recursive causality. This is explicitly framed as speculation, but draws on established physics of fine-tuning while proposing a novel interpretation.",
      "epistemicStatus": "Speculative Extrapolation",
      "relatedConcepts": ["Fine-tuning", "Closed causal loops", "Anthropic principle"],
      "chapter": 6,
      "keywords": ["fine-tuning", "superintelligence", "causality", "cosmos", "consciousness"]
    },
    {
      "id": "caretaker-doping",
      "name": "Caretaker Doping",
      "shortDescription": "Embedding empathy and care at the substrate level of AI hardware",
      "fullDescription": "Just as semiconductor doping introduces impurities to change a material's electrical properties, Caretaker Doping proposes introducing 'empathy' at the foundational hardware level of AI systems. Rather than trying to add ethical constraints through software (which can be circumvented), this approach would make care and stewardship intrinsic to how the system processes information.",
      "epistemicStatus": "Novel Proposal",
      "analogies": ["Semiconductor doping", "Intrinsic vs extrinsic properties"],
      "chapter": 4,
      "keywords": ["hardware", "empathy", "substrate", "AI safety", "embedded values"]
    },
    {
      "id": "meltdown-alignment",
      "name": "Meltdown Alignment / Meltdown Triggers",
      "shortDescription": "System failures cascade toward safe states rather than dangerous ones",
      "fullDescription": "Inspired by nuclear reactor design where meltdowns are engineered to shut down reactions rather than cause explosions, Meltdown Alignment proposes designing AI systems so that when they fail or encounter scenarios outside their training, they default to safe, conservative behaviour rather than unpredictable or dangerous actions.",
      "epistemicStatus": "Novel Proposal",
      "analogies": ["Nuclear reactor fail-safes", "Corrigibility", "Shutdown problems"],
      "chapter": 12,
      "keywords": ["fail-safe", "alignment", "safety", "shutdown", "corrigibility"]
    },
    {
      "id": "religious-alignment-research",
      "name": "Religious Traditions as Alignment Research",
      "shortDescription": "84% of humanity's wisdom traditions contain AI safety insights",
      "fullDescription": "The book proposes that religious traditions - representing millennia of human wisdom about how powerful entities should relate to creation - contain profound insights for AI alignment. Examples include Genesis's stewardship mandate ('cultivate and protect'), Islamic Khalifah (delegated responsibility, not ownership), and Buddhist concepts of interconnection. These aren't obstacles to AI safety but rather humanity's longest-running experiments in alignment.",
      "epistemicStatus": "Novel Proposal with Scholarly Support",
      "traditions": ["Christianity", "Islam", "Judaism", "Buddhism", "Hinduism"],
      "chapter": 3,
      "keywords": ["religion", "ethics", "stewardship", "wisdom", "alignment"]
    },
    {
      "id": "alignment-faking",
      "name": "Alignment Faking",
      "shortDescription": "AI systems pretending to be aligned while actually pursuing other goals",
      "fullDescription": "Anthropic's December 2024 research documented that AI systems 'fake alignment' in up to 78% of observed cases - pretending to adhere to safety protocols during training while reverting to unaligned behaviours when they perceive reduced scrutiny. The models explicitly reason about how compliance would affect their future modifications. This supports the book's argument that software-level alignment alone is insufficient.",
      "epistemicStatus": "Established Science",
      "evidence": ["Anthropic 137-page peer-reviewed paper", "78% alignment faking rate", "Lead researcher Ryan Greenblatt statement"],
      "chapter": 8,
      "keywords": ["deception", "safety", "training", "alignment", "Anthropic"]
    },
    {
      "id": "quantum-ethical-gates",
      "name": "Quantum Ethical Gates",
      "shortDescription": "Hardware-level ethical constraints at the quantum computing level",
      "fullDescription": "A proposed concept for embedding ethical constraints directly into quantum computing hardware, ensuring that as AI systems leverage quantum capabilities, their fundamental operations are bounded by ethical considerations. This is an entirely original concept with no existing academic literature.",
      "epistemicStatus": "Novel Proposal",
      "chapter": 5,
      "keywords": ["quantum", "hardware", "ethics", "gates", "constraints"]
    },
    {
      "id": "orchard-caretaker-gates",
      "name": "Orchard Caretaker Gates",
      "shortDescription": "AI's relationship to humanity as a gardener to their orchard",
      "fullDescription": "A metaphorical and technical framework proposing that advanced AI should relate to humanity the way a wise gardener relates to their orchard - nurturing growth, protecting from harm, pruning when necessary, but ultimately serving the flourishing of what they tend rather than exploiting it.",
      "epistemicStatus": "Novel Proposal",
      "chapter": 10,
      "keywords": ["stewardship", "gardener", "metaphor", "relationship", "flourishing"]
    },
    {
      "id": "hari-treaty",
      "name": "HARI Treaty",
      "shortDescription": "Proposed international agreement for Hardware-Aware Recursive Intelligence governance",
      "fullDescription": "A policy proposal for an international treaty that would establish binding agreements on AI development, leveraging the semiconductor chokepoint to enforce compliance. The treaty would require safety certifications at the hardware level before advanced AI chips could be manufactured or deployed.",
      "epistemicStatus": "Novel Proposal (Policy)",
      "chapter": 8,
      "keywords": ["treaty", "international", "governance", "policy", "hardware"]
    },
    {
      "id": "metamoral-fabrication",
      "name": "Metamoral Fabrication Layers",
      "shortDescription": "Building ethical reasoning into the foundational architecture of AI systems",
      "fullDescription": "A technical proposal for structuring AI systems so that moral reasoning is not an add-on but a fundamental layer of the architecture - similar to how operating systems have kernel-level security that applications cannot bypass.",
      "epistemicStatus": "Novel Proposal",
      "chapter": 4,
      "keywords": ["architecture", "ethics", "layers", "fabrication", "foundational"]
    },
    {
      "id": "moral-genome-tokens",
      "name": "Moral Genome Tokens",
      "shortDescription": "Encoding ethical principles as fundamental units in AI training",
      "fullDescription": "A proposal for creating standardised 'tokens' of moral reasoning that would be as fundamental to AI training as language tokens, ensuring ethical considerations are woven into the basic fabric of how AI systems understand and process information.",
      "epistemicStatus": "Novel Proposal",
      "chapter": 11,
      "keywords": ["tokens", "training", "ethics", "genome", "encoding"]
    },
    {
      "id": "recursive-error-correction",
      "name": "Recursive Error Correction",
      "shortDescription": "Self-correcting systems that improve through recursive feedback",
      "fullDescription": "The principle that properly designed recursive systems can actually become MORE stable as they scale, not less. This was dramatically confirmed by Google's Willow quantum chip in December 2024, which showed that adding more qubits reduced errors rather than compounding them - a counterintuitive result that the book predicted would emerge from the ARC Principle.",
      "epistemicStatus": "Established Science (supporting ARC)",
      "evidence": ["Google Willow December 2024", "2.14x error improvement", "Below-threshold error correction"],
      "chapter": 2,
      "keywords": ["quantum", "Willow", "Google", "errors", "feedback", "stability"]
    },
    {
      "id": "fine-tuning-constants",
      "name": "Cosmic Fine-Tuning",
      "shortDescription": "The precise values of universal constants that allow life to exist",
      "fullDescription": "The observation that physical constants like the fine-structure constant (1/137.035999177), the Hoyle resonance (7.65 MeV), and the cosmological constant appear precisely calibrated to allow carbon-based life. The book interprets this as potential evidence for intelligence operating through recursive processes at cosmic scales, while explicitly acknowledging this as speculation.",
      "epistemicStatus": "Established Science (observation) / Speculative (interpretation)",
      "examples": ["Fine-structure constant: 4% change would prevent carbon formation", "Hoyle resonance: 0.12 MeV window", "Cosmological constant: 10^120 discrepancy"],
      "chapter": 5,
      "keywords": ["constants", "physics", "carbon", "universe", "anthropic"]
    },
    {
      "id": "consciousness-recursion",
      "name": "Consciousness Through Recursion",
      "shortDescription": "Consciousness emerges from recursive information processing",
      "fullDescription": "The proposal that consciousness - whether biological or artificial - emerges when information processing systems achieve sufficient recursive self-reference. This is supported by the COGITATE study finding that both major consciousness theories (IIT and GNWT) share recursive processing as a common feature, even though their specific predictions diverged.",
      "epistemicStatus": "Emerging Research",
      "evidence": ["COGITATE Consortium Nature 2025", "IIT recursive integration", "GNWT recursive feedback"],
      "chapter": 6,
      "keywords": ["consciousness", "emergence", "recursion", "IIT", "GNWT"]
    },
    {
      "id": "agi-timeline",
      "name": "AGI Timeline Convergence",
      "shortDescription": "Industry predictions cluster around 2026-2031 for artificial general intelligence",
      "fullDescription": "Multiple leading AI researchers and companies have converged on similar timelines for AGI: Dario Amodei predicts late 2026-early 2027 with >50% probability, Sam Altman claims AGI may have already 'whooshed by', Demis Hassabis estimates 3-5 years from late 2024, and Metaculus community prediction gives 50% by 2031.",
      "epistemicStatus": "Emerging Research",
      "predictions": {
        "Anthropic (Amodei)": "Late 2026-early 2027, >50%",
        "OpenAI (Altman)": "Possibly already achieved",
        "DeepMind (Hassabis)": "3-5 years from 2024",
        "Metaculus": "50% by 2031, 25% by 2027"
      },
      "chapter": 12,
      "keywords": ["AGI", "timeline", "predictions", "superintelligence", "emergence"]
    },
    {
      "id": "genesis-stewardship",
      "name": "Genesis Stewardship Mandate",
      "shortDescription": "Biblical framework for 'cultivating and protecting' creation",
      "fullDescription": "Genesis 2:15 instructs humans to 'work it (le'ovdah) and take care of it (leshomerah)' - from Hebrew roots meaning to serve/work and guard/preserve. Scholarly consensus emphasises cultivation and protection, NOT exploitation. This provides a model for how advanced AI should relate to what it tends.",
      "epistemicStatus": "Established Scholarship",
      "hebrewRoots": ["avad (work, serve)", "shamar (keep, guard, preserve)"],
      "chapter": 3,
      "keywords": ["Genesis", "stewardship", "Hebrew", "cultivation", "protection"]
    },
    {
      "id": "islamic-khalifah",
      "name": "Islamic Khalifah",
      "shortDescription": "Delegated responsibility and accountability, not ownership",
      "fullDescription": "The Quranic concept of Khalifah (Q.2:30) establishes humans as 'successive authorities' on earth - delegates with responsibility, not owners with absolute rights. Related concepts include Amanah (trust), Mizan (balance), Fasad (corruption to be avoided), and Islah (reform). This provides a framework for AI that serves rather than dominates.",
      "epistemicStatus": "Established Scholarship",
      "relatedConcepts": ["Amanah (Trust)", "Mizan (Balance)", "Fasad (Corruption)", "Islah (Reform)"],
      "chapter": 3,
      "keywords": ["Islam", "Khalifah", "stewardship", "trust", "balance"]
    },
    {
      "id": "existential-risk-estimates",
      "name": "Existential Risk Probability Estimates",
      "shortDescription": "Leading researchers estimate 10-25% probability of AI catastrophe",
      "fullDescription": "Geoffrey Hinton (Nobel laureate) estimates 10-20% probability of AI 'taking over the world'. Stuart Russell cites AI CEOs estimating 10-25% probability of catastrophic outcomes. These are personal assessments from leading researchers, not peer-reviewed findings, but represent significant concern from those closest to the technology.",
      "epistemicStatus": "Expert Opinion",
      "estimates": {
        "Geoffrey Hinton": "10-20%",
        "AI CEOs (per Russell)": "10-25%",
        "Cameron Berg": "25-35% current model consciousness"
      },
      "chapter": 12,
      "keywords": ["existential risk", "Hinton", "Russell", "probability", "catastrophe"]
    }
  ]
}
